###########
# Data
###########

###########
# Model
###########
use_auth_token: true
tokenizer_max_length: 512
model_name: hc-opt-1.3b
accumulate_summary: true
segment_lengths: 2048

peft_model: lora
lora_r: 8
lora_alpha: 2
learning_rate: 0.0001
pooling_strategy: last
get_all_text_embeddings_batch_size: 1024

###########
# Training
###########
output_dir: output_dir
bf16: true
deepspeed: ds_configs/ds_opt.json
do_train: true
do_eval: true
do_predict: true
evaluation_strategy: steps
save_strategy: steps
metric_for_best_model: valid_accuracy
save_total_limit: 1
load_best_model_at_end: true
overwrite_output_dir: false

num_train_epochs: 200
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 16

eval_steps: 20
save_steps: 20
eval_delay: 50
logging_steps: 10

###########
# Others
###########
output_dir: output